{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Rg2uSiFSRJe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oDTSO7-MSRJj"
   },
   "source": [
    "# Problem 1\n",
    "## Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tx7KW28cSRJk"
   },
   "source": [
    "Write a function to **generate a training set** of size $m$\n",
    "- randomly generate a weight vector $w \\in \\mathbb{R}^{10}$, normalize length\n",
    "- generate a training set $\\{(x_i , y_i)\\}$ of size m\n",
    "  - $x_i$: random vector in $\\mathbb{R}^{10}$ from $\\textbf{N}(0, I)$\n",
    "  - $y_i$: $\\{0, +1\\}$ with $P[y = +1] = \\sigma(w \\cdot x_i)$ and $P[y = 0] = 1 - \\sigma(w \\cdot x_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AUHuXrsgSRJl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def generate_data(m):\n",
    "    # returns the true w as well as X, Y data\n",
    "\n",
    "    w = np.random.rand(10)\n",
    "    norm = np.linalg.norm(w)\n",
    "    global normalized_w\n",
    "    normalized_w = w / norm\n",
    "    training_set = np.zeros((m, 11))\n",
    "    \n",
    "    for i in range(m):\n",
    "        x = np.random.normal(0, 1, 10)\n",
    "        sigma = sigmoid(np.dot(normalized_w, x))\n",
    "        y_range = [0, 1]\n",
    "        y_prob = [1-sigma, sigma]\n",
    "        y = np.random.choice(a = y_range, p = y_prob)\n",
    "        training_set[i] = np.append(x,y)\n",
    "    return training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wx2-15fASRJy"
   },
   "source": [
    "## Algorithm 1: logistic regression\n",
    "\n",
    "The goal is to learn $w$.  Algorithm 1 is logistic\n",
    "  regression (you may use the built-in method LogisticRegression for this. Use max_iter=1000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vj8b21jgSRJz"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def logistic_regression_function(m_array):\n",
    "    average_difference_norm = np.zeros(len(m_array))\n",
    "    for idx, num in enumerate(m_array):\n",
    "        difference_norm = np.zeros(10)\n",
    "        for i in range(10):\n",
    "            training_set = generate_data(num)\n",
    "            X = training_set[:, :-1]\n",
    "            y = training_set[:, -1]\n",
    "            lr = LogisticRegression(max_iter=1000)\n",
    "            lr.fit(X, y)\n",
    "            global normalized_w\n",
    "            norm = np.linalg.norm(normalized_w - lr.coef_)\n",
    "            difference_norm[i]= norm\n",
    "        average_difference_norm[idx] = difference_norm.mean()\n",
    "    return average_difference_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YzmNdy6ZSRJ3"
   },
   "source": [
    "## Algorithm 2: gradient descent with square loss\n",
    "\n",
    "Define square loss as\n",
    "$$L_i(w^{(t)}) = \\frac{1}{2} \\left( \\sigma(w^{(t)} \\cdot x) - y_i \\right)^2$$\n",
    "\n",
    "  Algorithm 2 is\n",
    "  gradient descent with respect to square loss (code this\n",
    "  up yourself -- run for 1000 iterations, use step size eta = 0.01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.34035234, 0.87155809, 0.62210739, 0.59616378, 0.46144609])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_descent_with_square_loss(m_array):\n",
    "    average_difference_norm = np.zeros(len(m_array))\n",
    "    for idx, num in enumerate(m_array):\n",
    "        difference_norm = np.zeros(10)\n",
    "        for i in range(10):\n",
    "            training_set = generate_data(num)\n",
    "            X = training_set[:, :-1]\n",
    "            y = training_set[:, -1]\n",
    "            \n",
    "            w_backtick = np.random.randn(10)\n",
    "            step_size_eta = 0.01\n",
    "            iterations = 1000\n",
    "            for j in range(iterations):\n",
    "                predicted_y = sigmoid(np.dot(X, w_backtick))\n",
    "                gradient = np.dot(X.T, predicted_y - y)\n",
    "                gradient /= 10\n",
    "                gradient *= step_size_eta\n",
    "                w_backtick -= gradient \n",
    "            global normalized_w\n",
    "            norm = np.linalg.norm(normalized_w - w_backtick)\n",
    "            difference_norm[i]= norm\n",
    "        average_difference_norm[idx] = difference_norm.mean()\n",
    "    return average_difference_norm\n",
    "m_array = [50, 100, 150, 200, 250]\n",
    "#m_array = [2]\n",
    "gradient_descent_with_square_loss(m_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 3: stochastic gradient descent with square loss\n",
    "Similar to gradient descent, except we use the gradient at a single random training point every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5A-dLi3TSRJ-"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Measure error $\\|w - \\hat{w}\\|_2$ for each method at different sample size. For any\n",
    "  fixed value of $m$, choose many different $w$'s and average the\n",
    "  values $\\|w - \n",
    "  \\hat{w}\\|_2$ for Algorithms 1, 2 and 3.  Plot the results\n",
    "  for for each algorithm as you make $m$ large (use $m=50, 100, 150, 200, 250$).\n",
    "  Also record, for each algorithm, the time taken to run the overall experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.08655887 0.78456794 0.74136165 0.52659305 0.39129404]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "m_array = [50, 100, 150, 200, 250]\n",
    "start = time.process_time()\n",
    "print(logistic_regression_function(m_array))\n",
    "time_taken = time.process_time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each depth in $1, \\dots, 5$, instantiate an AdaBoost classifier with the base learner set to be a decision tree of that depth (set `n_estimators=10` and `learning_rate=1`), and then record the 10-fold cross-validated error on the entire breast cancer data set. Plot the resulting curve of accuracy against base classifier depth. Use $101$ as your random state for both the base learner as well as the AdaBoost classifier every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "hw2_programming_sol.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
