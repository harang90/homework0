{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Rg2uSiFSRJe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oDTSO7-MSRJj"
   },
   "source": [
    "# Problem 1\n",
    "## Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tx7KW28cSRJk"
   },
   "source": [
    "Write a function to **generate a training set** of size $m$\n",
    "- randomly generate a weight vector $w \\in \\mathbb{R}^{10}$, normalize length\n",
    "- generate a training set $\\{(x_i , y_i)\\}$ of size m\n",
    "  - $x_i$: random vector in $\\mathbb{R}^{10}$ from $\\textbf{N}(0, I)$\n",
    "  - $y_i$: $\\{0, +1\\}$ with $P[y = +1] = \\sigma(w \\cdot x_i)$ and $P[y = 0] = 1 - \\sigma(w \\cdot x_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AUHuXrsgSRJl",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.19989964 -1.83914429 -0.45465115  1.61865369 -1.17998095 -0.53371301\n",
      "  0.2239383  -0.58357109  0.55821271  1.11819061  0.        ]\n",
      "[-0.89142469  0.39409405  0.04025024 -0.30972668 -0.52991956 -0.47736486\n",
      "  0.23075343 -0.76914679 -0.33691911 -2.27781303  1.        ]\n",
      "[-0.4485122  -1.36012714  1.21600698  1.16810022  1.38558693  0.13671733\n",
      "  1.68092886  1.5917676  -0.23250359  0.32911066  1.        ]\n",
      "[ 0.59131877 -2.09049138  0.18891728 -0.58902606  0.52466882  1.54016992\n",
      "  0.83657185 -1.29084362  1.08230734  0.32400509  1.        ]\n",
      "[ 0.41392346  0.76000167 -1.67098984  0.6888266   1.1105941  -0.22035751\n",
      "  0.28614712 -0.03754744  1.44540026  1.09590197  1.        ]\n",
      "[ 0.15496343  1.04188345 -0.58051138  1.20562373  0.59515086  1.64347527\n",
      " -0.1139459   1.7121362  -0.97157952 -1.96810163  1.        ]\n",
      "[-0.09001957 -0.46575946  0.04621684  0.34583348  0.11648095 -0.21898948\n",
      " -2.03145783  0.72134005  0.43135723  1.5562934   1.        ]\n",
      "[-0.08520929  1.84504769 -0.18384071  1.02266497 -0.0501846  -0.33031795\n",
      "  0.89755382 -1.48330073  0.63288644 -0.06613517  0.        ]\n",
      "[-1.07151935 -0.80959909 -1.48590915  0.05682551  1.3941461  -0.26266741\n",
      " -1.3483967   0.31644118  0.46776387  1.44183151  1.        ]\n",
      "[ 1.18640844 -0.87329801 -0.96503971  1.60398086 -0.01493853  2.63316968\n",
      "  0.57002412 -0.56254563  1.1542713   1.04733306  1.        ]\n",
      "[-1.18426474  0.1737381  -1.31757453  0.24088719  0.62153722 -0.45567752\n",
      " -1.30291618  1.13006622  1.4815111  -1.34798192  1.        ]\n",
      "[ 1.84075424 -0.8514672   1.68853494 -0.06878804  0.28294358  0.87971737\n",
      "  0.78633031  0.84579722  0.59120114  1.50053746  1.        ]\n",
      "[-1.80613937 -0.31724124  0.24134913 -0.03732404 -0.97236582  0.32365722\n",
      " -0.69076126 -0.82313024  0.76174257  1.50991644  0.        ]\n",
      "[ 0.88501171  0.6077775  -0.70409575 -0.8762072  -0.6139551  -1.53770593\n",
      " -0.02103422  1.09311958 -0.87456967  0.91437887  0.        ]\n",
      "[-0.32295672 -0.67626092  0.50175765 -1.77345134  0.1962696   1.48089701\n",
      " -0.42823848 -0.35141446 -0.92699927  0.84509955  1.        ]\n",
      "[ 1.2072709  -1.17003604  1.43225857  0.23419238  0.17664553 -1.87488198\n",
      " -2.09569378 -1.13303623  2.00026396  1.41801281  0.        ]\n",
      "[-1.26929475  0.22687326  0.09515574 -0.07415788  2.93590219 -2.24316834\n",
      "  0.15768213  0.73645254 -0.66856719 -0.16832467  1.        ]\n",
      "[-0.15718148  1.06427058 -0.79854237  0.72132125  0.26858045 -0.82108428\n",
      " -0.09547693  0.51763006 -0.83716187  0.66178975  1.        ]\n",
      "[ 1.26131923 -0.35100085 -0.27771543 -0.53060995  1.42984095 -1.01100032\n",
      "  1.04937573  0.09303426 -0.18244158  0.36433067  0.        ]\n",
      "[-0.52485609  2.11432179 -0.11828317  0.90065781 -0.86083744 -1.28119971\n",
      "  1.3982961  -1.80117161 -0.13542754  0.55949895  0.        ]\n",
      "[ 0.20356415  1.64789144  2.0707807  -0.51825928  0.08158002 -0.42192704\n",
      "  0.68872323  0.83665148 -2.06968686 -0.32952546  1.        ]\n",
      "[ 1.27693464 -0.76594361 -2.9077988   0.33966868  0.2176235  -1.13052668\n",
      " -0.83135172  0.57169175  0.95137158 -0.81072794  0.        ]\n",
      "[ 0.42122488  0.50504035 -0.07259593 -1.32981804  0.5237455  -0.67011041\n",
      "  0.26890408  1.10918634  0.12867094  0.32551195  1.        ]\n",
      "[ 1.11651989 -0.0346962   0.10606155  0.71458895  0.7342172  -0.46591441\n",
      " -1.45133696  1.32030462 -1.09541411 -0.63865849  1.        ]\n",
      "[ 0.20398103 -0.09170368  1.43334383  0.24535192 -1.07732661 -0.04733812\n",
      " -0.68370709  0.83768649  0.33535142  0.24649637  1.        ]\n",
      "[ 0.06111396 -0.82564108 -1.21138455  0.38453078 -0.93375729  1.76162276\n",
      "  1.27173675  0.58722152  0.28105868 -1.04134749  0.        ]\n",
      "[ 0.59778434 -0.18403853  0.58601716 -0.75061043  0.6773288  -1.23136962\n",
      "  0.25676662 -0.62021716 -0.98442258  1.39699694  0.        ]\n",
      "[ 0.45350832 -1.85188292 -0.27684885  0.90296067 -0.25003277 -1.72680552\n",
      " -1.26101312  0.32437697 -1.0052774   0.33958781  0.        ]\n",
      "[-0.13252566  1.24262075  0.93294352 -0.6000765   1.51579955  1.31281619\n",
      " -1.68965232 -0.93949102  0.56743591  0.16368845  0.        ]\n",
      "[ 0.79851065 -0.78267429  0.14983299  0.05099727  0.32655327  1.57626116\n",
      " -1.10875566  0.36281007  0.56275047 -2.08167909  1.        ]\n",
      "[-1.00745633 -0.515597   -0.30094873 -0.3286312   0.99350269  2.83284202\n",
      " -1.15834669  0.52777469 -0.15909941 -0.7242857   1.        ]\n",
      "[-1.48325107  0.37957968  1.84100072  0.7101735  -1.73294851  0.12810929\n",
      "  1.18251925 -1.59901878  0.00833211  0.5146116   0.        ]\n",
      "[ 1.48029239 -0.24697307 -1.70989095 -0.02002425 -0.13450511  0.19012066\n",
      " -0.27770866  0.64110089 -0.56418337 -0.07606539  1.        ]\n",
      "[ 1.27207339  1.02851714  0.0875814   0.09848363  2.19308214 -0.84138335\n",
      "  1.21092765  0.22709954 -1.10174172 -1.07893813  0.        ]\n",
      "[ 0.37424567 -1.27557249 -1.08397936  0.47180834  0.14127779  2.45089374\n",
      "  1.22233593 -1.79183238  0.00572727 -0.31800776  0.        ]\n",
      "[ 1.39120163  0.52938713 -1.01531228  0.72486947 -0.27294079 -0.83237701\n",
      " -0.37185045 -0.10874575  2.41946013  0.10859626  1.        ]\n",
      "[ 0.44264688 -0.31171819  0.88560721 -0.59630584 -0.66077166 -0.57295941\n",
      "  0.70591352 -0.48727998  0.67395918  0.66754946  0.        ]\n",
      "[ 0.36439975 -1.2698584   0.29042842 -1.03033882 -0.50256425  0.40496402\n",
      "  2.48435488  0.5406774  -2.30089277  0.5120587   1.        ]\n",
      "[ 0.59826244 -1.08957684 -0.60531778 -1.19543533  0.86199567 -1.58201593\n",
      " -1.0648255   1.18701068  0.70481145  0.03538304  1.        ]\n",
      "[-0.96465806 -1.40985734 -0.27970122 -1.25560881  1.02553782 -1.26308913\n",
      "  1.22317381 -0.17873562 -0.6582067  -1.27222191  0.        ]\n",
      "[ 0.96099526  0.49279956  0.70740933  0.80278068 -1.10845904  0.82131694\n",
      " -1.26585715  0.21999265  0.54783491  1.05427686  1.        ]\n",
      "[-0.07152607  1.43721034  1.57181376  1.13683461 -0.05810584  0.99473848\n",
      "  0.63173386 -0.19234934 -0.6908606   0.76148107  1.        ]\n",
      "[ 0.65970542  0.96438456 -1.25034203 -0.32854321  0.12491239 -1.17371778\n",
      " -0.15699694 -1.55487177  0.96245427  1.16388144  0.        ]\n",
      "[-0.48605947 -1.3722746  -0.05575234 -0.77145236  1.37589067  1.67777849\n",
      " -1.5852114  -0.02547727 -2.9166349   0.93484101  0.        ]\n",
      "[ 0.22622836 -2.6719015   0.81357487  0.58969249 -1.18811824 -1.44620876\n",
      "  0.30432639  0.41569029 -0.58339618  0.82592033  0.        ]\n",
      "[-0.79134261  0.49167723  0.69837159  0.21373652  0.29903739  0.54272445\n",
      " -0.24723437  0.56582406 -0.28263042  1.47350152  1.        ]\n",
      "[-0.20761992 -1.25341324  0.49493238  2.69089464  0.085305   -0.77723088\n",
      " -0.01508283  0.20294387  0.23839105 -0.3095335   0.        ]\n",
      "[-0.26977965 -1.44564388 -2.55086099  0.40199722 -0.69728626  1.48019485\n",
      "  0.4377651   0.73770671 -0.17002642 -0.39907453  0.        ]\n",
      "[-1.39324524 -1.68794415 -0.46769227 -0.51353349  0.84086184  0.88964814\n",
      "  1.21496432  1.73212128  2.07846585  2.06721395  1.        ]\n",
      "[ 0.53587504  1.65695985 -0.84494202  2.61144127  1.68188787 -0.22412177\n",
      "  1.04337395  0.91369498 -0.3988521   0.80936753  1.        ]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def generate_data(m):\n",
    "    # returns the true w as well as X, Y data\n",
    "\n",
    "    w = np.random.rand(10)\n",
    "    norm = np.linalg.norm(w)\n",
    "    normalized_w = w / norm\n",
    "\n",
    "    training_set = np.zeros((m, 11))\n",
    "    \n",
    "    for i in range(m):\n",
    "        x = np.random.normal(0, 1, 10)\n",
    "        sigma = sigmoid(np.dot(normalized_w, x))\n",
    "        y_range = [0, 1]\n",
    "        y_prob = [1-sigma, sigma]\n",
    "        y = np.random.choice(a = y_range, p = y_prob)\n",
    "        print(np.append(x, y))\n",
    "        training_set[i] = np.append(x,y)\n",
    "    return training_set\n",
    "\n",
    "m_array = [50, 100, 150, 200, 250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wx2-15fASRJy"
   },
   "source": [
    "## Algorithm 1: logistic regression\n",
    "\n",
    "The goal is to learn $w$.  Algorithm 1 is logistic\n",
    "  regression (you may use the built-in method LogisticRegression for this. Use max_iter=1000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vj8b21jgSRJz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YzmNdy6ZSRJ3"
   },
   "source": [
    "## Algorithm 2: gradient descent with square loss\n",
    "\n",
    "Define square loss as\n",
    "$$L_i(w^{(t)}) = \\frac{1}{2} \\left( \\sigma(w^{(t)} \\cdot x) - y_i \\right)^2$$\n",
    "\n",
    "  Algorithm 2 is\n",
    "  gradient descent with respect to square loss (code this\n",
    "  up yourself -- run for 1000 iterations, use step size eta = 0.01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 3: stochastic gradient descent with square loss\n",
    "Similar to gradient descent, except we use the gradient at a single random training point every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5A-dLi3TSRJ-"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Measure error $\\|w - \\hat{w}\\|_2$ for each method at different sample size. For any\n",
    "  fixed value of $m$, choose many different $w$'s and average the\n",
    "  values $\\|w - \n",
    "  \\hat{w}\\|_2$ for Algorithms 1, 2 and 3.  Plot the results\n",
    "  for for each algorithm as you make $m$ large (use $m=50, 100, 150, 200, 250$).\n",
    "  Also record, for each algorithm, the time taken to run the overall experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each depth in $1, \\dots, 5$, instantiate an AdaBoost classifier with the base learner set to be a decision tree of that depth (set `n_estimators=10` and `learning_rate=1`), and then record the 10-fold cross-validated error on the entire breast cancer data set. Plot the resulting curve of accuracy against base classifier depth. Use $101$ as your random state for both the base learner as well as the AdaBoost classifier every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "hw2_programming_sol.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
